{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMISE12 prostate segmentation demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation:\n",
    "1) Make sure you have setup up the PROMISE12 data set. If not, see data/PROMISE12/README.md for instructions.\n",
    "\n",
    "2) Make sure you are in NiftyNet root, setting niftynet_path correctly to the path with the niftynet folder in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys \n",
    "niftynet_path=r'path/to/NiftyNet'\n",
    "os.chdir(niftynet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Make sure you have all the dependencies installed (replacing gpu with cpu for cpu-only mode):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "pip.main(['install','-r','requirements-gpu.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a network from the command line\n",
    "The simplest way to use NiftyNet is via the commandline net_segmentation.py script. Normally, this is done on the command line with a command like this from the NiftyNet root directory:\n",
    "\n",
    "```python net_segmentation.py train --conf demo/PROMISE12/promise12_demo_train_config.txt --image_size 32 --label_size 32 --max_iter 10```\n",
    "\n",
    "Notice that we use configuration file that is specific to this experiment. This file contains default settings. Also note that we can override these settings on the command line.\n",
    "\n",
    "To execute NiftyNet from within the notebook, you can run the following python code:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import niftynet\n",
    "sys.argv=['','train', '--conf',os.path.join('demos','PROMISE12','promise12_demo_train_config.txt'),'--image_size','32','--label_size','32','--max_iter','10']\n",
    "niftynet.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have trained (a few iterations of) a deep learning network for medical image segmentation. If you have some time on your hands, you can finish training the network (by leaving off the max_iter argument) and try it out, by running the following command\n",
    "\n",
    "```python net_segmentation.py inference --conf demo/PROMISE12/promise12_demo_inference_config.txt --image_size 32 --label_size 32```\n",
    "\n",
    "or the following python code in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import niftynet\n",
    "sys.argv=['', 'inference','--conf',os.path.join('demos','PROMISE12','promise12_demo_train_config.txt'),'--image_size','32','--label_size','32']\n",
    "niftynet.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, you can load up some pre-trained weights for the network:\n",
    "\n",
    "```python net_segmentation.py inference --conf demo/PROMISE12/promise12_demo_config.txt --model_dir demo/PROMISE12/pretrained```\n",
    "or the following python code in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import niftynet\n",
    "sys.argv=['', 'inference', '--conf',os.path.join('demos','PROMISE12','promise12_demo_inference_config.txt'), '--model_dir', os.path.join('demos','PROMISE12','pretrained')]\n",
    "niftynet.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find your segmented images in output/promise12_demo\n",
    "\n",
    "NiftyNet has taken care of a lot of details behind the scenes:\n",
    "1. Organizing data into a dataset of images and segmentation labels\n",
    "2. Building a deep leaning network (in this case, it is based on VNet by Milletari et al.)\n",
    "3. Added deep learning infrastruture, such as a loss function for segmentation, the ADAM optimizer.\n",
    "4. Added augmentation, where the images are zoomed and rotated a little bit for every training step so that you do not over-fit the data\n",
    "5. Run the training algorithm\n",
    "\n",
    "All of this was controlled by the configuration file.\n",
    "\n",
    "## The configuration file\n",
    "\n",
    " Let's take a closer look at the configuration file. Further details about the configuration settings are available in ```config/readme.md```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[image modality 1]\n",
    "path_to_search = ./data/PROMISE12\n",
    "filename_contains = T2\n",
    "filename_not_contains = Case2\n",
    "\n",
    "[label modality 1]\n",
    "path_to_search = ./data/PROMISE12\n",
    "filename_contains = segmentation\n",
    "filename_not_contains = Case2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines define how NiftyNet organizes your data. In this case, in the ./data/PROMISE12 folder there is one T2-weighted MR image named 'Case??_T2.nii.gz' and one reference segmentation named 'Case??_segmentation.nii.gz' per patient. The images for each patient are automatically grouped because they share the same prefix 'Case??'. For training, we exclude patients Case20-Case26, and for inference, we only include patients Case20-Case26, so that our training and inference data are mutually exclusive."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[settings]\n",
    "cuda_devices = \"\"\n",
    "model_dir = ./models/model_vnet\n",
    "# preprocessing threads parameters\n",
    "queue_length = 8\n",
    "num_threads = 1\n",
    "# ** training only ** system parameters\n",
    "num_gpus = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines are setting up some system parameters: which GPUs to use (in this case whatever is available), where to save the trained network parameters, how many images to queue up in advance so that the GPU isn't waiting for data, and how many threads to use for queuing them up."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# network parameters (controlling memory consumption)\n",
    "net_name = vnet\n",
    "spatial_rank = 3\n",
    "batch_size = 1\n",
    "image_size = 128\n",
    "label_size = 128\n",
    "w_map_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "# affected by network's receptive field\n",
    "volume_padding_size = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines are setting parameters for the network itself."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#histogram normalisation\n",
    "histogram_ref_file = ./example_volumes/monomodal_parcellation/standardisation_models.txt\n",
    "multimod_mask_type = and\n",
    "norm_type = percentile\n",
    "cutoff_min = 0.01\n",
    "cutoff_max = 0.99\n",
    "mask_type = otsu_plus\n",
    "\n",
    "# volume level preprocessing\n",
    "reorientation = True\n",
    "resampling = True\n",
    "normalisation = True\n",
    "whitening = True\n",
    "image_interp_order = 3\n",
    "label_interp_order = 0\n",
    "w_map_interp_order = 3\n",
    "\n",
    "# training augmentation\n",
    "rotation = True\n",
    "min_angle = -10.0\n",
    "max_angle = 10.0\n",
    "spatial_scaling = True\n",
    "min_percentage = -10.0\n",
    "max_percentage = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines are setting parameters for preprocessing and data augmentation. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sample_per_volume = 1\n",
    "# ** training only ** gradient descent and loss parameters\n",
    "lr = 0.0001\n",
    "decay = 1e-7\n",
    "loss_type = Dice\n",
    "reg_type = L2\n",
    "starting_iter = 0\n",
    "save_every_n = 5000\n",
    "max_iter = 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines are setting parameters for the inference algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ** inference only ** system parameters\n",
    "border = 5\n",
    "pred_iter = 25000\n",
    "save_seg_dir = ./output/promise12_demo\n",
    "\n",
    "output_interp_order = 0\n",
    "output_prob = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the demo interactively\n",
    "To see the inner workings of NiftyNet, we can run this demo within the Notebook interactively. A simple working example, (with many NiftyNet features left out) follows.\n",
    "\n",
    "Begin by importing libraries, including tensorflow, core NiftyNet libraries and helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from network.vnet import VNet # This module is the network architecture we will use for segmentation\n",
    "from utilities.csv_table import CSVTable,SubjectTable # This class holds the subject ids and file names\n",
    "from engine.volume_loader import VolumeLoaderLayer # This module loads sets of images to feed into the network\n",
    "import engine.training\n",
    "import utilities.parse_user_params as parse_user_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use helper functions to parse commandline parameters and automatically match patient data (which is what net_segmentation.py did)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv=['','train', '--conf',os.path.join('demos','PROMISE12','promise12_demo_train_config.txt'),'--image_size','32','--label_size','32','--max_iter','10']\n",
    "param, csv_dict = parse_user_params.run()\n",
    "csv_loader = CSVTable(csv_dict=csv_dict, allow_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use write your own code to change parameters and specify the patients to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = parse_user_params.default_params('train','./demos/PROMISE12/promise12_demo_train_config.txt')\n",
    "params.max_iter=10\n",
    "loader = SubjectTable([['Case%02d'%i,[['./data/PROMISE12/Case%02d_T2.nii.gz'%i]],[['./data/PROMISE12/Case%02d_segmentation.nii.gz'%i]],'',''] for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the form of the parameter passed to SubjectTable. It is a list, where each element represents one subject. Each element is a list where the first element is the id of the patient, the second element is a list of lists of image names representing the images being segmented. The list of lists is flexible enough to define multiple modalities and multiple time points if that suits your segmentation problem. The third element is a list of lists representing the segmentation labels. The fourth and fifth elements are blanks, but for some problems could represent a weighting image, and additional notes each subject. \n",
    "\n",
    "Finally, create the data loader and run the training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_loader = VolumeLoaderLayer(loader)\n",
    "engine.training.run(VNet, params, volume_loader, 'gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple snippet already illustrates several key concepts in NiftyNet:\n",
    "1) NiftyNet is built of various modules: pre-built networks, like VNet; the training module which executes the training; the VolumeLoader which loads images and structures them into minibatches. \n",
    "2) These components are parameterized by a common object, which can be read from configuration files and manipulated in the code.\n",
    "## Summary\n",
    "\n",
    "In this demo \n",
    "1. you learned to run training and testing for a deep-learning-based segmentation pipeline from the command-line and from python code directly; \n",
    "2. you also learned about the NiftyNet configuration files, and how they control the learning and inference process; and \n",
    "3. you learned multiple ways to tell NiftyNet which data to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
